{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III clinical note classification using doc2vec (gensim) and a gated recurrent unit neural network (tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates an approach to classifying medical notes by type (physician vs. social worker) using deep learning. The clinical notes were obtained through MIMIC-III, a publically availabele ICU database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "import tensorflow as tf\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from random import shuffle\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import functools\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tables from SQL server and convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See README file for details on obtaining the MIMIC csv files, and see the SQL build file for details on converting the tables to SQL. This project is limited to the entire corpus of the social worker notes (2670 total documents) and an equal number of physician notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = MySQLdb.connect(host=\"0.0.0.0\",    # your host, usually localhost\n",
    "                     user=\"user\",         # your username\n",
    "                     passwd=\"passwd\",  # your password\n",
    "                     db=\"mimic_sql\")        # name of the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert SQL tables for pandas dataframe\n",
    "\n",
    "physician_df = pd.read_sql(\"SELECT ROW_ID, CATEGORY, TEXT FROM NOTEEVENTS where category='Physician' limit 2670\", con=db)\n",
    "sw_df = pd.read_sql(\"SELECT ROW_ID, CATEGORY, TEXT FROM NOTEEVENTS where category='Social Work'\", con=db)\n",
    "\n",
    "# Combine physician and social worker notes into a single dataframe\n",
    "notes = pd.concat([physician_df, sw_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import sentence embeddings. See sentence_embeddings_mod.py for details\n",
    "fname = 'd2v-200'\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text from notes into 3D numpy array with zero padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gated recurrent unit neural network outlined below requires the training/testing data to be in 3D np array with dimensions equal to [total # of training examples, max length # of sentence vectors, sentence vector length]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drewclyde/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:16: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/home/drewclyde/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c8bdba3c2a05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcube\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CATEGORY'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Social Work'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtotal_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/drewclyde/anaconda3/lib/python3.5/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mdstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_replace_zero_by_x_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_arys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Set max_length to maximum # of sentences from splitting training text\n",
    "max_length = 200\n",
    "\n",
    "# create two empty lists for training parameters and target variables\n",
    "y = []\n",
    "X = []\n",
    "\n",
    "# iterate through notes dataframe and \n",
    "for index, row in notes.iterrows():\n",
    "    # for each clinical note, tokenize into sentences\n",
    "    line_array = sent_tokenize(row['TEXT'])\n",
    "    matrix = []\n",
    "\n",
    "    for sentence in line_array:\n",
    "        # for each sentence remove all non-letters and split by white space\n",
    "        sentence = re.sub('[^A-Za-z\\s]+', ' ', sentence).lower().replace(\"\\n\",\" \").split()\n",
    "        if sentence == []:\n",
    "            pass\n",
    "        elif matrix == []:\n",
    "            matrix = model.infer_vector(sentence)\n",
    "        else:\n",
    "            # create 2D matrix with dimension [max_length, sentence vector length]\n",
    "            matrix = np.vstack((matrix, np.array(model.infer_vector(sentence))))\n",
    "    if len(matrix.shape) < 2:\n",
    "        matrix = np.vstack((matrix, np.zeros(max_length)))\n",
    "    while matrix.shape[0] < 200:\n",
    "        matrix = np.vstack((matrix, np.zeros(max_length)))\n",
    "    if X == []:\n",
    "        X = matrix\n",
    "    else:\n",
    "        # stack 2D parameter arrays into 3D array\n",
    "        X = np.dstack((cube,matrix))\n",
    "    if row['CATEGORY']=='Social Work':\n",
    "    # convert target variable to one-hot 2D array\n",
    "        total_y.append(np.array([0,1]))\n",
    "    else:\n",
    "        total_y.append(np.array([1,0]))\n",
    "\n",
    "y = np.vstack(y)\n",
    "\n",
    "# adjust parameter dimensions\n",
    "X = np.swapaxes(X,1,2)\n",
    "X = np.swapaxes(X,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_X, total_y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating, training, and testing GRU/RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was adapted from a variable length sequencing tutorial written by Danijar Hafner described [here](https://danijar.com/variable-sequence-lengths-in-tensorflow/) and shown in full [here](https://gist.github.com/danijar/d11c77c5565482e965d1919291044470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drewclyde/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 31.0%\n",
      "Epoch  2 error 8.0%\n",
      "Epoch  3 error 8.0%\n",
      "Epoch  4 error 7.0%\n",
      "Epoch  5 error 6.0%\n",
      "Epoch  6 error 4.0%\n",
      "Epoch  7 error 4.0%\n",
      "Epoch  8 error 5.0%\n",
      "Epoch  9 error 5.0%\n",
      "Epoch 10 error 12.0%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceClassification:\n",
    "    # initialize variables\n",
    "    def __init__(self, data, target, num_hidden=200, num_layers=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    # function for trimming zero padding from input 2D arrays\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # building recurrent network\n",
    "        output, _ = tf.nn.dynamic_rnn(\n",
    "            tf.contrib.rnn.GRUCell(self._num_hidden),\n",
    "            data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "        # Softmax layer\n",
    "        weight, bias = self._weight_and_bias(\n",
    "            self._num_hidden, int(self.target.get_shape()[1]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.003\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        # function for selecting only last target for training\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We treat images as sequences of pixel rows.\n",
    "    examples, max_length, vec_size = X_train.shape\n",
    "    num_classes = y_train.shape[1]\n",
    "    batch_size = 10 \n",
    "    data = tf.placeholder(tf.float32, [None, max_length, vec_size])\n",
    "    target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "    model = VariableSequenceClassification(data, target)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        for _ in range(0,examples,batch_size):\n",
    "            batch_X, batch_y = X_train[_:_+batch_size], y_train[_:_+batch_size]\n",
    "            sess.run(model.optimize, {\n",
    "                data: batch_X, target: batch_y})\n",
    "        error = sess.run(model.error, {data: X_test, target: y_test})\n",
    "        print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
